{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/finngen-tools/blob/main/text_generation_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation example\n",
        "\n",
        "This is a brief example of how to run text generation with a Finnish GPT-3 model on Colab.\n",
        "\n",
        "Install [transformers](https://huggingface.co/docs/transformers/index) python package. This will be used to load the model and tokenizer and to run generation."
      ],
      "metadata": {
        "id": "tIQ1s96UCcJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers"
      ],
      "metadata": {
        "id": "4fUBJmXHCHw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the `AutoTokenizer` and `AutoModelForCausalLM` classes. These support loading tokenizers and generative models from the [Hugging Face repository](https://huggingface.co/models)."
      ],
      "metadata": {
        "id": "5ZRNZgRJCt6Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwyK005xCFSF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a generative model and its tokenizer. You can substitute any other generative model name here (e.g. [other TurkuNLP GPT-3 models](https://huggingface.co/models?sort=downloads&search=turkunlp%2Fgpt3)), but note that Colab may have issues running larger models."
      ],
      "metadata": {
        "id": "6QJPDe3ZC_sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'TurkuNLP/gpt3-finnish-large'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "wqTxn_QaCNjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simple function to generate text using broadly reasonable parameters. For details on text generation using `transformers`, see e.g. [this tutorial](https://huggingface.co/blog/how-to-generate)."
      ],
      "metadata": {
        "id": "4Hox0r9XDk7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, temperature=0.7, max_new_tokens=50):\n",
        "    input = tokenizer(prompt, return_tensors='pt')\n",
        "    output = model.generate(\n",
        "        **input,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        no_repeat_ngram_size=2,\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "C_YFKAXXCap5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few generation examples. Note that re-running these generation examples will produce different outputs as `model.generate` is invoked with the `do_sample=True` parameter.\n",
        "\n",
        "First, a simple sentence continuation:"
      ],
      "metadata": {
        "id": "4CFtlezpEKzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate('Suomen kieltä käsittelevien tekoälymenetelmien kehitykselle on keskeisen tärkeää, että'))"
      ],
      "metadata": {
        "id": "-G_DOpmsEJnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of one-shot classification:"
      ],
      "metadata": {
        "id": "bSprUQvnFk9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Onko seuraava lause positiivinen vai negatiivinen?\n",
        "Lause: Täällä on erittäin hauskaa!\n",
        "Vastaus: positiivinen\n",
        "\n",
        "Onko seuraava lause positiivinen vai negatiivinen?\n",
        "Lause: elokuva oli surkea!\n",
        "Vastaus:\"\"\"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "id": "seNYjM53E8Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note above that even if the model produces the right answer, it usually doesn't know to stop generation after the answer and attempts to predict a plausible continuation."
      ],
      "metadata": {
        "id": "BGfS1jGOF2Bf"
      }
    }
  ]
}